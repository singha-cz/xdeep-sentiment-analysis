{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10faaf5",
   "metadata": {},
   "source": [
    "# Anal√Ωza sentimentu textu pomoc√≠ LSTM\n",
    "\n",
    "Tento notebook implementuje model pro automatickou anal√Ωzu sentimentu text≈Ø pomoc√≠ hlubok√© neuronov√© s√≠tƒõ (LSTM).\n",
    "\n",
    "**C√≠l projektu:** Bin√°rn√≠ klasifikace sentimentu (pozitivn√≠/negativn√≠) na datasetu Sentiment140.\n",
    "\n",
    "## Obsah\n",
    "1. Nastaven√≠ prost≈ôed√≠ a import knihoven\n",
    "2. Naƒçten√≠ a explorace dat\n",
    "3. Preprocessing textu\n",
    "4. Tokenizace a p≈ô√≠prava dat\n",
    "5. Vytvo≈ôen√≠ LSTM modelu\n",
    "6. Tr√©nov√°n√≠ modelu\n",
    "7. Vyhodnocen√≠ a vizualizace\n",
    "8. Inference pro nov√© texty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c048d7",
   "metadata": {},
   "source": [
    "## 1. Nastaven√≠ prost≈ôed√≠\n",
    "\n",
    "Nejprve ovƒõ≈ô√≠me p≈ôipojen√≠ ke Google Colab a zkontrolujeme dostupn√Ω hardware (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44cc0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ovƒõ≈ôen√≠ prost≈ôed√≠ - zjist√≠me, jestli bƒõ≈æ√≠me na Colab a jak√Ω m√°me hardware\n",
    "import sys\n",
    "\n",
    "# Kontrola, zda bƒõ≈æ√≠me v Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Bƒõ≈æ√≠me v Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ö†Ô∏è Nebƒõ≈æ√≠me v Colab - pou≈æ√≠v√°me lok√°ln√≠ prost≈ôed√≠\")\n",
    "\n",
    "# Kontrola dostupn√©ho GPU\n",
    "import tensorflow as tf\n",
    "print(f\"\\nüì¶ TensorFlow verze: {tf.__version__}\")\n",
    "print(f\"üêç Python verze: {sys.version}\")\n",
    "\n",
    "# Kontrola GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"üöÄ GPU dostupn√©: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"   - {gpu.name}\")\n",
    "else:\n",
    "    print(\"üíª GPU nen√≠ dostupn√© - pou≈æijeme CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f8a6e",
   "metadata": {},
   "source": [
    "### Import knihoven\n",
    "\n",
    "Nyn√≠ naimportujeme v≈°echny pot≈ôebn√© knihovny:\n",
    "- **TensorFlow/Keras** - framework pro deep learning\n",
    "- **NumPy** - numerick√© v√Ωpoƒçty\n",
    "- **Pandas** - pr√°ce s daty\n",
    "- **scikit-learn** - metriky a rozdƒõlen√≠ dat\n",
    "- **Matplotlib** - vizualizace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import v≈°ech pot≈ôebn√Ωch knihoven\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re  # pro ƒçi≈°tƒõn√≠ textu (regul√°rn√≠ v√Ωrazy)\n",
    "\n",
    "# TensorFlow a Keras komponenty\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Scikit-learn pro metriky a rozdƒõlen√≠ dat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Nastaven√≠ pro reprodukovatelnost v√Ωsledk≈Ø\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"‚úÖ V≈°echny knihovny √∫spƒõ≈°nƒõ naimportov√°ny!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4246f0b",
   "metadata": {},
   "source": [
    "## 2. Naƒçten√≠ dat\n",
    "\n",
    "Dataset **Sentiment140** obsahuje 1.6 milionu tweet≈Ø oznaƒçen√Ωch jako pozitivn√≠ (4) nebo negativn√≠ (0).\n",
    "\n",
    "St√°hneme ho z Kaggle pomoc√≠ knihovny `kagglehub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1195b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sta≈æen√≠ datasetu Sentiment140 z Kaggle\n",
    "# P≈ôi prvn√≠m spu≈°tƒõn√≠ m≈Ø≈æe b√Ωt pot≈ôeba autentizace\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Sta≈æen√≠ datasetu - kagglehub ho ulo≈æ√≠ do cache\n",
    "print(\"üì• Stahuji dataset Sentiment140...\")\n",
    "dataset_path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
    "print(f\"‚úÖ Dataset sta≈æen do: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naƒçten√≠ datasetu do pandas DataFrame\n",
    "import os\n",
    "\n",
    "# Najdeme CSV soubor v datasetu\n",
    "csv_file = os.path.join(dataset_path, \"training.1600000.processed.noemoticon.csv\")\n",
    "\n",
    "# Dataset nem√° hlaviƒçky, mus√≠me je definovat ruƒçnƒõ\n",
    "column_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Naƒçten√≠ CSV - encoding je latin-1 (ISO-8859-1)\n",
    "print(\"üìÇ Naƒç√≠t√°m data do pamƒõti...\")\n",
    "df = pd.read_csv(csv_file, encoding='latin-1', names=column_names)\n",
    "\n",
    "print(f\"‚úÖ Naƒçteno {len(df):,} z√°znam≈Ø\")\n",
    "print(f\"üìä Sloupce: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76e33c",
   "metadata": {},
   "source": [
    "### Explorace dat\n",
    "\n",
    "Pod√≠v√°me se, jak data vypadaj√≠ ‚Äî prvn√≠ch p√°r ≈ô√°dk≈Ø, rozlo≈æen√≠ sentimentu, p≈ô√≠klady text≈Ø."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e354d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zobrazen√≠ prvn√≠ch 5 ≈ô√°dk≈Ø\n",
    "print(\"üìã Prvn√≠ch 5 z√°znam≈Ø:\")\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a34367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozlo≈æen√≠ sentimentu v datasetu\n",
    "print(\"üìä Rozlo≈æen√≠ sentimentu:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "# Vizualizace\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "df['target'].value_counts().plot(kind='bar', ax=ax, color=['#ff6b6b', '#51cf66'])\n",
    "ax.set_xticklabels(['Negativn√≠ (0)', 'Pozitivn√≠ (4)'], rotation=0)\n",
    "ax.set_ylabel('Poƒçet')\n",
    "ax.set_title('Rozlo≈æen√≠ sentimentu v datasetu')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P≈ô√≠klady negativn√≠ch a pozitivn√≠ch tweet≈Ø\n",
    "print(\"üò† P≈ô√≠klady NEGATIVN√çCH tweet≈Ø:\")\n",
    "for text in df[df['target'] == 0]['text'].head(3):\n",
    "    print(f\"  ‚Ä¢ {text[:100]}...\")\n",
    "\n",
    "print(\"\\nüòä P≈ô√≠klady POZITIVN√çCH tweet≈Ø:\")\n",
    "for text in df[df['target'] == 4]['text'].head(3):\n",
    "    print(f\"  ‚Ä¢ {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca1633",
   "metadata": {},
   "source": [
    "## 3. Preprocessing textu\n",
    "\n",
    "Tweety obsahuj√≠ mnoho \"≈°umu\", kter√Ω model nepot≈ôebuje:\n",
    "- **@mentions** ‚Äî u≈æivatelsk√° jm√©na\n",
    "- **URL odkazy** ‚Äî http://...\n",
    "- **Speci√°ln√≠ znaky** ‚Äî emotikony, HTML entity\n",
    "- **Velk√° p√≠smena** ‚Äî p≈ôevedeme na mal√°\n",
    "\n",
    "Vytvo≈ô√≠me funkci pro ƒçi≈°tƒõn√≠ textu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb3e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Vyƒçist√≠ tweet od ≈°umu.\n",
    "    \n",
    "    Args:\n",
    "        text: P≈Øvodn√≠ text tweetu\n",
    "    Returns:\n",
    "        Vyƒçi≈°tƒõn√Ω text\n",
    "    \"\"\"\n",
    "    # P≈ôevod na mal√° p√≠smena\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Odstranƒõn√≠ @mentions (nap≈ô. @user123)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Odstranƒõn√≠ URL odkaz≈Ø\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Odstranƒõn√≠ HTML entit (nap≈ô. &amp; &lt;)\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    \n",
    "    # Odstranƒõn√≠ speci√°ln√≠ch znak≈Ø a ƒç√≠slic (ponech√°me jen p√≠smena a mezery)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Odstranƒõn√≠ nadbyteƒçn√Ωch mezer\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test funkce na p≈ô√≠kladu\n",
    "test_tweet = \"@user123 I love this movie!!! üòä Check it out: http://example.com #awesome\"\n",
    "print(f\"P≈Øvodn√≠:   '{test_tweet}'\")\n",
    "print(f\"Vyƒçi≈°tƒõn√Ω: '{clean_text(test_tweet)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplikace ƒçi≈°tƒõn√≠ na cel√Ω dataset\n",
    "# POZOR: Toto m≈Ø≈æe trvat nƒõkolik minut na 1.6M z√°znamech!\n",
    "\n",
    "print(\"üßπ ƒåist√≠m texty...\")\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "print(\"‚úÖ Hotovo!\")\n",
    "\n",
    "# Uk√°zka p≈ôed/po\n",
    "print(\"\\nüìä Uk√°zka vyƒçi≈°tƒõn√Ωch text≈Ø:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nP≈Øvodn√≠:   {df['text'].iloc[i][:80]}...\")\n",
    "    print(f\"Vyƒçi≈°tƒõn√Ω: {df['clean_text'].iloc[i][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67648f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P≈ôevod sentimentu na bin√°rn√≠ hodnoty (0 = negativn√≠, 1 = pozitivn√≠)\n",
    "# P≈Øvodnƒõ: 0 = negativn√≠, 4 = pozitivn√≠\n",
    "# Novƒõ:    0 = negativn√≠, 1 = pozitivn√≠\n",
    "\n",
    "df['sentiment'] = df['target'].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "print(\"üìä Nov√© rozlo≈æen√≠ sentimentu:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(f\"\\n0 = Negativn√≠, 1 = Pozitivn√≠\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d9734",
   "metadata": {},
   "source": [
    "## 4. Tokenizace a p≈ô√≠prava dat\n",
    "\n",
    "Neuronov√° s√≠≈• nerozum√≠ textu ‚Äî pot≈ôebuje ƒç√≠sla. Proto mus√≠me:\n",
    "\n",
    "1. **Tokenizace** ‚Äî ka≈æd√© slovo dostane unik√°tn√≠ ƒç√≠slo (index)\n",
    "2. **Padding** ‚Äî v≈°echny sekvence mus√≠ m√≠t stejnou d√©lku\n",
    "\n",
    "P≈ô√≠klad: `\"i love this\"` ‚Üí `[1, 45, 23]` ‚Üí `[1, 45, 23, 0, 0, 0, ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametry pro tokenizaci\n",
    "VOCAB_SIZE = 20000      # Maxim√°ln√≠ poƒçet slov ve slovn√≠ku\n",
    "MAX_LENGTH = 100        # Maxim√°ln√≠ d√©lka sekvence (tweetu)\n",
    "EMBEDDING_DIM = 128     # Dimenze embedding vektoru\n",
    "\n",
    "print(f\"üìä Nastaven√≠:\")\n",
    "print(f\"   Velikost slovn√≠ku: {VOCAB_SIZE:,} slov\")\n",
    "print(f\"   Max. d√©lka tweetu: {MAX_LENGTH} token≈Ø\")\n",
    "print(f\"   Embedding dimenze: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e04825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvo≈ôen√≠ tokenizeru a tr√©nov√°n√≠ na na≈°ich datech\n",
    "print(\"üî§ Vytv√°≈ô√≠m tokenizer a slovn√≠k...\")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "\n",
    "# Informace o slovn√≠ku\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"‚úÖ Slovn√≠k vytvo≈ôen!\")\n",
    "print(f\"   Celkem unik√°tn√≠ch slov: {len(word_index):,}\")\n",
    "print(f\"   Pou≈æijeme top {VOCAB_SIZE:,} nejƒçastƒõj≈°√≠ch\")\n",
    "\n",
    "# Uk√°zka nƒõkolika slov a jejich index≈Ø\n",
    "print(\"\\nüìñ Uk√°zka slovn√≠ku (slovo ‚Üí index):\")\n",
    "for word, idx in list(word_index.items())[:10]:\n",
    "    print(f\"   '{word}' ‚Üí {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P≈ôevod text≈Ø na sekvence ƒç√≠sel\n",
    "print(\"üî¢ P≈ôev√°d√≠m texty na sekvence ƒç√≠sel...\")\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
    "\n",
    "# Uk√°zka p≈ôevodu\n",
    "print(\"\\nüìù Uk√°zka p≈ôevodu text ‚Üí sekvence:\")\n",
    "print(f\"   Text: '{df['clean_text'].iloc[0][:50]}...'\")\n",
    "print(f\"   Sekvence: {sequences[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebde61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding - doplnƒõn√≠/zkr√°cen√≠ na stejnou d√©lku\n",
    "print(f\"üìè Aplikuji padding na d√©lku {MAX_LENGTH}...\")\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_LENGTH, padding='pre', truncating='post')\n",
    "y = df['sentiment'].values\n",
    "\n",
    "print(f\"‚úÖ Data p≈ôipravena!\")\n",
    "print(f\"   Tvar X (vstupy): {X.shape}\")\n",
    "print(f\"   Tvar y (labely): {y.shape}\")\n",
    "\n",
    "# Uk√°zka paddovan√© sekvence\n",
    "print(f\"\\nüìù Uk√°zka paddovan√© sekvence:\")\n",
    "print(f\"   D√©lka: {len(X[0])}\")\n",
    "print(f\"   Prvn√≠ch 20 hodnot: {X[0][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ebe3d",
   "metadata": {},
   "source": [
    "### Rozdƒõlen√≠ dat\n",
    "\n",
    "Rozdƒõl√≠me data na 3 ƒç√°sti:\n",
    "- **Training (80%)** ‚Äî pro tr√©nov√°n√≠ modelu\n",
    "- **Validation (10%)** ‚Äî pro sledov√°n√≠ bƒõhem tr√©nov√°n√≠\n",
    "- **Test (10%)** ‚Äî pro fin√°ln√≠ vyhodnocen√≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozdƒõlen√≠ na train a temp (test + validation)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,      # 20% pro test+val\n",
    "    random_state=42,    # Pro reprodukovatelnost\n",
    "    stratify=y          # Zachov√° pomƒõr t≈ô√≠d\n",
    ")\n",
    "\n",
    "# Rozdƒõlen√≠ temp na validation a test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,      # 50% z 20% = 10% celkem\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data rozdƒõlena!\")\n",
    "print(f\"   Train:      {X_train.shape[0]:,} vzork≈Ø ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"   Validation: {X_val.shape[0]:,} vzork≈Ø ({X_val.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"   Test:       {X_test.shape[0]:,} vzork≈Ø ({X_test.shape[0]/len(X)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a72a9",
   "metadata": {},
   "source": [
    "## 5. Vytvo≈ôen√≠ LSTM modelu\n",
    "\n",
    "Architektura modelu:\n",
    "\n",
    "```\n",
    "Input (sekvence ƒç√≠sel) \n",
    "    ‚Üì\n",
    "Embedding (slova ‚Üí vektory)\n",
    "    ‚Üì\n",
    "LSTM (zachycen√≠ kontextu)\n",
    "    ‚Üì\n",
    "Dropout (prevence p≈ôetr√©nov√°n√≠)\n",
    "    ‚Üì\n",
    "Dense + Sigmoid (v√Ωstup 0-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b4308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvo≈ôen√≠ LSTM modelu\n",
    "model = Sequential([\n",
    "    # Embedding vrstva - p≈ôevede indexy slov na hust√© vektory\n",
    "    # input_dim = velikost slovn√≠ku, output_dim = dimenze vektoru\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM),\n",
    "    \n",
    "    # LSTM vrstva - zachyt√≠ kontext a vztahy mezi slovy\n",
    "    # 64 = poƒçet LSTM jednotek (neuron≈Ø)\n",
    "    LSTM(64, return_sequences=False),\n",
    "    \n",
    "    # Dropout - n√°hodnƒõ \"vypne\" 50% neuron≈Ø p≈ôi tr√©nov√°n√≠\n",
    "    # Pom√°h√° proti overfittingu (p≈ôeuƒçen√≠)\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # V√Ωstupn√≠ vrstva - 1 neuron s sigmoid aktivac√≠\n",
    "    # Sigmoid vrac√≠ hodnotu 0-1 (pravdƒõpodobnost pozitivn√≠ho sentimentu)\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Kompilace modelu\n",
    "model.compile(\n",
    "    optimizer='adam',                    # Optimalizaƒçn√≠ algoritmus\n",
    "    loss='binary_crossentropy',          # Loss funkce pro bin√°rn√≠ klasifikaci\n",
    "    metrics=['accuracy']                 # Metrika pro sledov√°n√≠\n",
    ")\n",
    "\n",
    "# Vybudov√°n√≠ modelu s konkr√©tn√≠m tvarem vstupu\n",
    "model.build(input_shape=(None, MAX_LENGTH))\n",
    "\n",
    "# Zobrazen√≠ architektury\n",
    "print(\"üìê Architektura modelu:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0c5f6",
   "metadata": {},
   "source": [
    "## 6. Tr√©nov√°n√≠ modelu\n",
    "\n",
    "Nyn√≠ budeme model tr√©novat. Pou≈æijeme:\n",
    "- **EarlyStopping** ‚Äî zastav√≠ tr√©nov√°n√≠, kdy≈æ se model p≈ôestane zlep≈°ovat\n",
    "- **Validation data** ‚Äî pro sledov√°n√≠, jak model generalizuje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a19c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nastaven√≠ tr√©nov√°n√≠\n",
    "EPOCHS = 5              # Poƒçet pr≈Øchod≈Ø cel√Ωm datasetem\n",
    "BATCH_SIZE = 256        # Poƒçet vzork≈Ø v jedn√© d√°vce\n",
    "\n",
    "# Ukl√°d√°n√≠ modelu - nejprve lok√°ln√≠, potom Google Drive jako fallback\n",
    "model_dir = 'models'\n",
    "print(\"üìÅ V√Ωchoz√≠ adres√°≈ô: lok√°ln√≠ disk (models/)\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "        model_dir = '/content/drive/My Drive/xdeep-sentiment/models'\n",
    "        print(\"‚úÖ Google Drive √∫spƒõ≈°nƒõ p≈ôipojen - bude se pou≈æ√≠vat jako fallback\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Nepoda≈ôilo se p≈ôipojit Google Drive: {e}\")\n",
    "        print(\"üìÅ Budu pou≈æ√≠vat lok√°ln√≠ adres√°≈ô: models/\")\n",
    "else:\n",
    "    print(\"üíª Nebƒõ≈æ√≠m v Colab\")\n",
    "    \n",
    "import os\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "print(f\"üìÅ Modely se budou ukl√°dat do: {model_dir}\")\n",
    "\n",
    "# Early stopping - zastav√≠ tr√©nov√°n√≠ pokud se val_loss nezlep≈°√≠ 2 epochy za sebou\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',      # Sledujeme validation loss\n",
    "    patience=2,              # Poƒçet epoch bez zlep≈°en√≠ p≈ôed zastaven√≠m\n",
    "    restore_best_weights=True # Vr√°t√≠ v√°hy z nejlep≈°√≠ epochy\n",
    ")\n",
    "\n",
    "# ModelCheckpoint - automaticky ukl√°d√° model po ka≈æd√© epo≈°e\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f'{model_dir}/sentiment_lstm_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,     # Ulo≈æ√≠ pouze nejlep≈°√≠ model\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"üìä Nastaven√≠ tr√©nov√°n√≠:\")\n",
    "print(f\"   Poƒçet epoch: {EPOCHS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Train vzork≈Ø: {len(X_train):,}\")\n",
    "print(f\"   Krok≈Ø na epochu: {len(X_train) // BATCH_SIZE:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c57a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tr√©nov√°n√≠ modelu\n",
    "\n",
    "print(\"üöÄ Zaƒç√≠n√°m tr√©nov√°n√≠...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop, checkpoint],  # P≈ôid√°n checkpoint!\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Tr√©nov√°n√≠ dokonƒçeno!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47081623",
   "metadata": {},
   "source": [
    "## 7. Vyhodnocen√≠ a vizualizace\n",
    "\n",
    "Pod√≠v√°me se, jak model tr√©noval a jak si vede na testovac√≠ch datech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdad167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizace pr≈Øbƒõhu tr√©nov√°n√≠\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Graf Loss (ztr√°ta)\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "axes[0].set_title('Pr≈Øbƒõh Loss bƒõhem tr√©nov√°n√≠')\n",
    "axes[0].set_xlabel('Epocha')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Graf Accuracy (p≈ôesnost)\n",
    "axes[1].plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "axes[1].set_title('Pr≈Øbƒõh Accuracy bƒõhem tr√©nov√°n√≠')\n",
    "axes[1].set_xlabel('Epocha')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# V√Ωpis nejlep≈°√≠ch hodnot\n",
    "best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "print(f\"\\nüìä Nejlep≈°√≠ epocha: {best_epoch}\")\n",
    "print(f\"   Val Loss: {history.history['val_loss'][best_epoch-1]:.4f}\")\n",
    "print(f\"   Val Accuracy: {history.history['val_accuracy'][best_epoch-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df629b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vyhodnocen√≠ modelu na testovac√≠ch datech\n",
    "print(\"üß™ Vyhodnocuji model na testovac√≠ch datech...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Predikce na testovac√≠ch datech\n",
    "y_pred_prob = model.predict(X_test, verbose=0)  # Pravdƒõpodobnosti (0-1)\n",
    "\n",
    "# Diagnostika - pod√≠v√°me se na rozlo≈æen√≠ predikc√≠\n",
    "print(f\"\\nüîç Diagnostika predikc√≠:\")\n",
    "print(f\"   Min pravdƒõpodobnost: {y_pred_prob.min():.6f}\")\n",
    "print(f\"   Max pravdƒõpodobnost: {y_pred_prob.max():.6f}\")\n",
    "print(f\"   Pr≈Ømƒõr: {y_pred_prob.mean():.6f}\")\n",
    "print(f\"   Prvn√≠ch 10 predikc√≠: {y_pred_prob[:10].flatten()}\")\n",
    "\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()  # P≈ôevod na 0/1\n",
    "\n",
    "print(f\"\\n   Poƒçet predikovan√Ωch 0 (negativn√≠): {(y_pred == 0).sum():,}\")\n",
    "print(f\"   Poƒçet predikovan√Ωch 1 (pozitivn√≠): {(y_pred == 1).sum():,}\")\n",
    "print(f\"   Skuteƒçn√Ωch 0: {(y_test == 0).sum():,}\")\n",
    "print(f\"   Skuteƒçn√Ωch 1: {(y_test == 1).sum():,}\")\n",
    "\n",
    "# V√Ωpoƒçet metrik\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nüìä Metriky na testovac√≠ch datech:\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negativn√≠', 'Pozitivn√≠']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1755473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pokus o naƒçten√≠ ulo≈æen√©ho modelu (pokud existuje)\n",
    "import os\n",
    "\n",
    "# Pou≈æijeme stejnou cestu jako p≈ôi tr√©nov√°n√≠\n",
    "if IN_COLAB:\n",
    "    model_path = '/content/drive/My Drive/xdeep-sentiment/models/sentiment_lstm_model.keras'\n",
    "else:\n",
    "    model_path = 'models/sentiment_lstm_model.keras'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"‚úÖ Nalezen ulo≈æen√Ω model: {model_path}\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(\"‚úÖ Model √∫spƒõ≈°nƒõ naƒçten!\")\n",
    "\n",
    "    model.summary()    \n",
    "    print(\"‚ö†Ô∏è Spus≈• nejprve tr√©nov√°n√≠ (bu≈àka 28).\")\n",
    "\n",
    "else:    print(f\"‚ùå Model nebyl nalezen: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05eb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizace Confusion Matrix (matice z√°mƒõn)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "# Popisky\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Negativn√≠', 'Pozitivn√≠'])\n",
    "ax.set_yticklabels(['Negativn√≠', 'Pozitivn√≠'])\n",
    "ax.set_xlabel('Predikce')\n",
    "ax.set_ylabel('Skuteƒçnost')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# P≈ôid√°n√≠ hodnot do bunƒõk\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, f'{cm[i, j]:,}', ha='center', va='center', \n",
    "                       color='white' if cm[i, j] > cm.max()/2 else 'black', fontsize=14)\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretace\n",
    "print(f\"\\nüìä Interpretace Confusion Matrix:\")\n",
    "print(f\"   ‚úÖ True Negatives (spr√°vnƒõ negativn√≠):  {cm[0,0]:,}\")\n",
    "print(f\"   ‚ùå False Positives (fale≈°nƒõ pozitivn√≠): {cm[0,1]:,}\")\n",
    "print(f\"   ‚ùå False Negatives (fale≈°nƒõ negativn√≠): {cm[1,0]:,}\")\n",
    "print(f\"   ‚úÖ True Positives (spr√°vnƒõ pozitivn√≠):  {cm[1,1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99275941",
   "metadata": {},
   "source": [
    "## 8. Inference pro nov√© texty\n",
    "\n",
    "Vytvo≈ô√≠me funkci, kter√° umo≈æn√≠ predikovat sentiment pro libovoln√Ω nov√Ω text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model=model, tokenizer=tokenizer, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Predikuje sentiment pro dan√Ω text.\n",
    "    \n",
    "    Args:\n",
    "        text: Text k anal√Ωze (string)\n",
    "        model: Natr√©novan√Ω model\n",
    "        tokenizer: Tokenizer pou≈æit√Ω p≈ôi tr√©nov√°n√≠\n",
    "        max_length: Maxim√°ln√≠ d√©lka sekvence\n",
    "        \n",
    "    Returns:\n",
    "        dict s predikc√≠, pravdƒõpodobnost√≠ a interpretac√≠\n",
    "    \"\"\"\n",
    "    # Vyƒçi≈°tƒõn√≠ textu (stejn√° funkce jako p≈ôi tr√©nov√°n√≠)\n",
    "    cleaned = clean_text(text)\n",
    "    \n",
    "    # Tokenizace\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned])\n",
    "    \n",
    "    # Padding - MUS√ç b√Ωt stejn√Ω jako p≈ôi tr√©nov√°n√≠ (padding='pre')\n",
    "    padded = pad_sequences(sequence, maxlen=max_length, padding='pre', truncating='post')\n",
    "    \n",
    "    # Predikce\n",
    "    probability = model.predict(padded, verbose=0)[0][0]\n",
    "    sentiment = 'Pozitivn√≠ üòä' if probability > 0.5 else 'Negativn√≠ üò†'\n",
    "    confidence = probability if probability > 0.5 else 1 - probability\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'cleaned': cleaned,\n",
    "        'sentiment': sentiment,\n",
    "        'probability': probability,\n",
    "        'confidence': confidence * 100\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funkce predict_sentiment() je p≈ôipravena!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f4ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testov√°n√≠ na vlastn√≠ch textech\n",
    "test_texts = [\n",
    "    \"I absolutely love this product! Best purchase ever!\",\n",
    "    \"This is terrible. Waste of money. Never buying again.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"I'm so excited about the weekend trip!\",\n",
    "    \"My flight was cancelled and I'm stuck at the airport.\",\n",
    "    \"Just had an amazing dinner with friends!\",\n",
    "    \"This movie was boring and predictable.\",\n",
    "    \"This was the best night of my life. But on the other hand, it was the worst night of my life.\"\n",
    "]\n",
    "\n",
    "print(\"üîÆ Testov√°n√≠ modelu na nov√Ωch textech:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_sentiment(text)\n",
    "    print(f\"\\nüìù \\\"{text[:50]}{'...' if len(text) > 50 else ''}\\\"\")\n",
    "    print(f\"   ‚Üí {result['sentiment']} (jistota: {result['confidence']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7da13",
   "metadata": {},
   "source": [
    "## 9. Ulo≈æen√≠ modelu\n",
    "\n",
    "Ulo≈æ√≠me natr√©novan√Ω model pro pozdƒõj≈°√≠ pou≈æit√≠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ulo≈æen√≠ modelu a tokenizeru (na Google Drive pro perzistenci)\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Pou≈æijeme stejnou cestu jako p≈ôi tr√©nov√°n√≠\n",
    "if IN_COLAB:\n",
    "    save_dir = '/content/drive/My Drive/xdeep-sentiment/models'\n",
    "else:\n",
    "    save_dir = 'models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Ulo≈æen√≠ Keras modelu (nativn√≠ form√°t)\n",
    "model_path = os.path.join(save_dir, 'sentiment_lstm_model.keras')\n",
    "model.save(model_path)\n",
    "print(f\"‚úÖ Model ulo≈æen: {model_path}\")\n",
    "\n",
    "# Ulo≈æen√≠ tokenizeru (pot≈ôebujeme ho pro inference)\n",
    "tokenizer_path = os.path.join(save_dir, 'tokenizer.pkl')\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(f\"‚úÖ Tokenizer ulo≈æen: {tokenizer_path}\")\n",
    "\n",
    "# Ulo≈æen√≠ konfigurace\n",
    "config = {\n",
    "    'VOCAB_SIZE': VOCAB_SIZE,\n",
    "    'MAX_LENGTH': MAX_LENGTH,\n",
    "    'EMBEDDING_DIM': EMBEDDING_DIM\n",
    "}\n",
    "config_path = os.path.join(save_dir, 'config.pkl')\n",
    "with open(config_path, 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "\n",
    "print(f\"‚úÖ Konfigurace ulo≈æena: {config_path}\")\n",
    "print(\"üí° Tyto soubory p≈ôe≈æij√≠ restart Colab runtime!\")\n",
    "\n",
    "print(f\"\\nüìÅ V≈°echny soubory jsou na Google Drive: {save_dir}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733bb015",
   "metadata": {},
   "source": [
    "## Shrnut√≠\n",
    "\n",
    "√öspƒõ≈°nƒõ jsme vytvo≈ôili model pro anal√Ωzu sentimentu! \n",
    "\n",
    "**Co jsme se nauƒçili:**\n",
    "- Naƒç√≠t√°n√≠ a p≈ôedzpracov√°n√≠ textov√Ωch dat\n",
    "- Tokenizace a padding sekvenc√≠\n",
    "- Vytvo≈ôen√≠ a tr√©nov√°n√≠ LSTM modelu\n",
    "- Vyhodnocen√≠ pomoc√≠ metrik a confusion matrix\n",
    "- Inference pro nov√© texty\n",
    "\n",
    "**Mo≈æn√° vylep≈°en√≠:**\n",
    "- Pou≈æ√≠t `padding='pre'` m√≠sto `'post'` (lep≈°√≠ pro LSTM)\n",
    "- P≈ôidat `Bidirectional(LSTM(...))` pro obousmƒõrn√© zpracov√°n√≠\n",
    "- Experimentovat s jin√Ωmi architekturami (GRU, Transformer)\n",
    "- Fine-tuning na specifick√© dom√©nƒõ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
